{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\geethanjaliexternship'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras==2.2.4\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras==2.2.4) (1.4.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras==2.2.4) (5.1.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras==2.2.4) (1.1.2)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras==2.2.4) (1.12.0)\n",
      "Collecting keras-applications>=1.0.6 (from keras==2.2.4)\n",
      "  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "Requirement already satisfied: h5py in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras==2.2.4) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras==2.2.4) (1.16.4)\n",
      "Installing collected packages: keras-applications, keras\n",
      "  Found existing installation: Keras 2.4.0\n",
      "    Uninstalling Keras-2.4.0:\n",
      "      Successfully uninstalled Keras-2.4.0\n",
      "Successfully installed keras-2.2.4 keras-applications-1.0.8\n",
      "Collecting tensorflow==1.14.0\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/08/25e47a53692c2e0dcd2211a493ddfe9007a5cd92e175d6dffa6169a0b392/tensorflow-1.14.0-cp37-cp37m-win_amd64.whl (68.3MB)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow==1.14.0) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow==1.14.0) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow==1.14.0) (1.16.4)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow==1.14.0) (1.11.2)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow==1.14.0) (0.33.4)\n",
      "Requirement already satisfied: gast>=0.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow==1.14.0) (0.3.3)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow==1.14.0) (1.38.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow==1.14.0) (0.12.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow==1.14.0) (3.17.2)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow==1.14.0) (1.12.0)\n",
      "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow==1.14.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
      "Collecting astor>=0.6.0 (from tensorflow==1.14.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
      "Collecting tensorboard<1.15.0,>=1.14.0 (from tensorflow==1.14.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow==1.14.0) (1.1.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow==1.14.0) (1.0.8)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.3.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (41.0.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (0.15.4)\n",
      "Requirement already satisfied: h5py in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\hp\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (0.17)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (0.5.1)\n",
      "Installing collected packages: tensorflow-estimator, astor, tensorboard, tensorflow\n",
      "  Found existing installation: tensorflow-estimator 2.3.0\n",
      "    Uninstalling tensorflow-estimator-2.3.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
      "  Found existing installation: tensorboard 2.5.0\n",
      "    Uninstalling tensorboard-2.5.0:\n",
      "      Successfully uninstalled tensorboard-2.5.0\n",
      "  Found existing installation: tensorflow 2.3.0\n",
      "    Uninstalling tensorflow-2.3.0:\n",
      "      Successfully uninstalled tensorflow-2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\users\\\\hp\\\\anaconda3\\\\lib\\\\site-packages\\\\~ensorflow\\\\lite\\\\experimental\\\\microfrontend\\\\python\\\\ops\\\\_audio_microfrontend_op.so'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.2.4\n",
    "!pip install tensorflow==1.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras import layers \n",
    "\n",
    "from tensorflow.keras.layers import Dense,Flatten\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D \n",
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4-tf'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorflow.keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Data Agumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\n",
    "test_datagen=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'botocore'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-ce451c4e0d03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mibm_boto3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'botocore'"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share the notebook.\n",
    "\n",
    "if os.environ.get('RUNTIME_ENV_LOCATION_TYPE') == 'external':\n",
    "    endpoint_7d01c3a53d364926a1681543db957042 = 'https://s3-api.us-geo.objectstorage.softlayer.net'\n",
    "else:\n",
    "    endpoint_7d01c3a53d364926a1681543db957042 = 'https://s3-api.us-geo.objectstorage.service.networklayer.com'\n",
    "\n",
    "client_7d01c3a53d364926a1681543db957042 = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='JgJzfbbo7B9qxmS1RCP6a2RGNgKAMWVIv1X_vnNos2XO',\n",
    "    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url=endpoint_7d01c3a53d364926a1681543db957042)\n",
    "\n",
    "# Your data file was loaded into a botocore.response.StreamingBody object.\n",
    "# Please read the documentation of ibm_boto3 and pandas to learn more about the possibilities to load the data.\n",
    "# ibm_boto3 documentation: https://ibm.github.io/ibm-cos-sdk-python/\n",
    "# pandas documentation: http://pandas.pydata.org/\n",
    "streaming_body_10 = client_7d01c3a53d364926a1681543db957042.get_object(Bucket='imageclassification-donotdelete-pr-r0zoamtbaszqeg', Key='dataset.zip')['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(streaming_body_10, \"__iter__\"): streaming_body_10.__iter__ = types.MethodType( __iter__, streaming_body_10 ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'streaming_body_10' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-83002ebb7027>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mio\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0munzip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstreaming_body_10\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mfile_paths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamelist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile_paths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'streaming_body_10' is not defined"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import zipfile\n",
    "unzip = zipfile.ZipFile(BytesIO(streaming_body_10.read()),'r')\n",
    "file_paths=unzip.namelist()\n",
    "for path in file_paths:\n",
    "    unzip.extract(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/wsuser/work'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "filenames = os.listdir('/home/wsuser/work/dataset/train_set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading our data and performing data agumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/home/wsuser/work/dataset/train_set'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e226c98f8983>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m x_train = train_datagen.flow_from_directory(\"/home/wsuser/work/dataset/train_set\",target_size=(64, 64),batch_size=5,\n\u001b[1;32m----> 2\u001b[1;33m                                             color_mode='rgb',class_mode=\"categorical\")\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m x_test = test_datagen.flow_from_directory(\"/home/wsuser/work/dataset/test_set\",target_size=(64, 64),batch_size=5,\n\u001b[0;32m      5\u001b[0m                                             color_mode='rgb',class_mode=\"categorical\")\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[1;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[0;32m    541\u001b[0m             \u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m             \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 543\u001b[1;33m             \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    544\u001b[0m         )\n\u001b[0;32m    545\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\directory_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/home/wsuser/work/dataset/train_set'"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train = train_datagen.flow_from_directory(\"/home/wsuser/work/dataset/train_set\",target_size=(64, 64),batch_size=5,\n",
    "                                            color_mode='rgb',class_mode=\"categorical\")\n",
    "\n",
    "x_test = test_datagen.flow_from_directory(\"/home/wsuser/work/dataset/test_set\",target_size=(64, 64),batch_size=5,\n",
    "                                            color_mode='rgb',class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cyclone': 0, 'Earthquake': 1, 'Flood': 2, 'Wildfire': 3}\n"
     ]
    }
   ],
   "source": [
    "print(x_train.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cyclone': 0, 'Earthquake': 1, 'Flood': 2, 'Wildfire': 3}\n"
     ]
    }
   ],
   "source": [
    "print(x_test.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 220, 1: 156, 2: 198, 3: 168})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter as c\n",
    "c(x_train .labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# Initializing the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# First convolution layer and pooling\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape=(64, 64, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Second convolution layer and pooling\n",
    "classifier.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "# input_shape is going to be the pooled feature maps from the previous convolution layer\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flattening the layers\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Adding a fully connected layer\n",
    "classifier.add(Dense(units=128, activation='relu'))\n",
    "classifier.add(Dense(units=4, activation='softmax')) # softmax for more than 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 29, 29, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 813,604\n",
      "Trainable params: 813,604\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "149/149 [==============================] - 32s 218ms/step - loss: 1.2729 - acc: 0.4124 - val_loss: 1.2354 - val_acc: 0.3636\n",
      "Epoch 2/10\n",
      "149/149 [==============================] - 30s 202ms/step - loss: 0.9520 - acc: 0.5849 - val_loss: 0.9171 - val_acc: 0.6616\n",
      "Epoch 3/10\n",
      "149/149 [==============================] - 29s 194ms/step - loss: 0.7683 - acc: 0.6604 - val_loss: 0.6776 - val_acc: 0.7475\n",
      "Epoch 4/10\n",
      "149/149 [==============================] - 29s 197ms/step - loss: 0.6508 - acc: 0.7466 - val_loss: 0.7413 - val_acc: 0.7374\n",
      "Epoch 5/10\n",
      "149/149 [==============================] - 29s 194ms/step - loss: 0.6546 - acc: 0.7345 - val_loss: 0.7710 - val_acc: 0.6768\n",
      "Epoch 6/10\n",
      "149/149 [==============================] - 29s 195ms/step - loss: 0.6056 - acc: 0.7453 - val_loss: 0.6295 - val_acc: 0.7424\n",
      "Epoch 7/10\n",
      "149/149 [==============================] - 30s 199ms/step - loss: 0.5392 - acc: 0.7817 - val_loss: 0.7014 - val_acc: 0.7323\n",
      "Epoch 8/10\n",
      "149/149 [==============================] - 29s 194ms/step - loss: 0.4947 - acc: 0.8154 - val_loss: 0.6578 - val_acc: 0.7424\n",
      "Epoch 9/10\n",
      "149/149 [==============================] - 30s 200ms/step - loss: 0.4658 - acc: 0.8181 - val_loss: 0.6177 - val_acc: 0.7727\n",
      "Epoch 10/10\n",
      "149/149 [==============================] - 29s 197ms/step - loss: 0.3919 - acc: 0.8518 - val_loss: 0.8023 - val_acc: 0.7323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2d14499750>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " classifier.fit_generator(\n",
    "        generator=x_train,steps_per_epoch = len(x_train),\n",
    "        epochs=10, validation_data=x_test,validation_steps = len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# saving our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('disaster.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disaster.h5\r\n"
     ]
    }
   ],
   "source": [
    "!tar -zcvf image-classification-model_new.tgz disaster.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mdataset\u001b[0m/\r\n",
      "disaster.h5\r\n",
      "image-classification-model_new.tgz\r\n",
      "my_model.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "ls -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already up-to-date: watson-machine-learning-client in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (1.0.391)\n",
      "Requirement already satisfied, skipping upgrade: lomond in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from watson-machine-learning-client) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: ibm-cos-sdk in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from watson-machine-learning-client) (2.7.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from watson-machine-learning-client) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from watson-machine-learning-client) (2020.12.5)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from watson-machine-learning-client) (1.0.5)\n",
      "Requirement already satisfied, skipping upgrade: tabulate in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from watson-machine-learning-client) (0.8.3)\n",
      "Requirement already satisfied, skipping upgrade: urllib3 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from watson-machine-learning-client) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from watson-machine-learning-client) (1.13.11)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from watson-machine-learning-client) (4.47.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from lomond->watson-machine-learning-client) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: ibm-cos-sdk-s3transfer==2.7.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from ibm-cos-sdk->watson-machine-learning-client) (2.7.0)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from ibm-cos-sdk->watson-machine-learning-client) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: ibm-cos-sdk-core==2.7.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from ibm-cos-sdk->watson-machine-learning-client) (2.7.0)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests->watson-machine-learning-client) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests->watson-machine-learning-client) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pandas->watson-machine-learning-client) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pandas->watson-machine-learning-client) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pandas->watson-machine-learning-client) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from boto3->watson-machine-learning-client) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.17.0,>=1.16.11 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from boto3->watson-machine-learning-client) (1.16.11)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from ibm-cos-sdk-core==2.7.0->ibm-cos-sdk->watson-machine-learning-client) (0.15.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install watson-machine-learning-client --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning import APIClient\n",
    "wml_credentials={\"url\":\"https://us-south.ml.cloud.ibm.com\",\n",
    "                \"apikey\":\"jgOtjSk8dNpif80Z8fXmgYY6Yk4C_qs4xvKoFedjwRaH\"}\n",
    "client = APIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = APIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guid_space_name(client,space_name):\n",
    "    space=client.spaces.get_details()\n",
    "    return(next(item for item in space['resources'] if item['entity'][\"name\"]==space_name)['metadata']['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "space UID = 747c36a0-5ec5-4348-9a7b-ec6bb50b63af\n"
     ]
    }
   ],
   "source": [
    "space_uid=guid_space_name(client,'imageclassification')\n",
    "print(\"space UID = \"+space_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SUCCESS'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.set.default_space(space_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------  ------------------------------------  ----\n",
      "NAME                           ASSET_ID                              TYPE\n",
      "default_py3.6                  0062b8c9-8b7d-44a0-a9b9-46c416adcbd9  base\n",
      "pytorch-onnx_1.3-py3.7-edt     069ea134-3346-5748-b513-49120e15d288  base\n",
      "scikit-learn_0.20-py3.6        09c5a1d0-9c1e-4473-a344-eb7b665ff687  base\n",
      "spark-mllib_3.0-scala_2.12     09f4cff0-90a7-5899-b9ed-1ef348aebdee  base\n",
      "ai-function_0.1-py3.6          0cdb0f1e-5376-4f4d-92dd-da3b69aa9bda  base\n",
      "shiny-r3.6                     0e6e79df-875e-4f24-8ae9-62dcc2148306  base\n",
      "pytorch_1.1-py3.6              10ac12d6-6b30-4ccd-8392-3e922c096a92  base\n",
      "tensorflow_1.15-py3.6-ddl      111e41b3-de2d-5422-a4d6-bf776828c4b7  base\n",
      "scikit-learn_0.22-py3.6        154010fa-5b3b-4ac1-82af-4d5ee5abbc85  base\n",
      "default_r3.6                   1b70aec3-ab34-4b87-8aa0-a4a3c8296a36  base\n",
      "pytorch-onnx_1.3-py3.6         1bc6029a-cc97-56da-b8e0-39c3880dbbe7  base\n",
      "tensorflow_2.1-py3.6           1eb25b84-d6ed-5dde-b6a5-3fbdf1665666  base\n",
      "tensorflow_1.15-py3.6          2b73a275-7cbf-420b-a912-eae7f436e0bc  base\n",
      "pytorch_1.2-py3.6              2c8ef57d-2687-4b7d-acce-01f94976dac1  base\n",
      "spark-mllib_2.3                2e51f700-bca0-4b0d-88dc-5c6791338875  base\n",
      "pytorch-onnx_1.1-py3.6-edt     32983cea-3f32-4400-8965-dde874a8d67e  base\n",
      "spark-mllib_3.0-py37           36507ebe-8770-55ba-ab2a-eafe787600e9  base\n",
      "spark-mllib_2.4                390d21f8-e58b-4fac-9c55-d7ceda621326  base\n",
      "xgboost_0.82-py3.6             39e31acd-5f30-41dc-ae44-60233c80306e  base\n",
      "pytorch-onnx_1.2-py3.6-edt     40589d0e-7019-4e28-8daa-fb03b6f4fe12  base\n",
      "spark-mllib_2.4-r_3.6          49403dff-92e9-4c87-a3d7-a42d0021c095  base\n",
      "xgboost_0.90-py3.6             4ff8d6c2-1343-4c18-85e1-689c965304d3  base\n",
      "pytorch-onnx_1.1-py3.6         50f95b2a-bc16-43bb-bc94-b0bed208c60b  base\n",
      "spark-mllib_2.4-scala_2.11     55a70f99-7320-4be5-9fb9-9edb5a443af5  base\n",
      "autoai-obm_2.0                 5c2e37fa-80b8-5e77-840f-d912469614ee  base\n",
      "spss-modeler_18.1              5c3cad7e-507f-4b2a-a9a3-ab53a21dee8b  base\n",
      "autoai-kb_3.1-py3.7            632d4b22-10aa-5180-88f0-f52dfb6444d7  base\n",
      "spark-mllib_2.3-r_3.6          6586b9e3-ccd6-4f92-900f-0f8cb2bd6f0c  base\n",
      "tensorflow_2.4-py3.7           65e171d7-72d1-55d9-8ebb-f813d620c9bb  base\n",
      "spss-modeler_18.2              687eddc9-028a-4117-b9dd-e57b36f1efa5  base\n",
      "pytorch-onnx_1.2-py3.6         692a6a4d-2c4d-45ff-a1ed-b167ee55469a  base\n",
      "do_12.9                        75a3a4b0-6aa0-41b3-a618-48b1f56332a6  base\n",
      "spark-mllib_2.3-scala_2.11     7963efe5-bbec-417e-92cf-0574e21b4e8d  base\n",
      "spark-mllib_2.4-py37           7abc992b-b685-532b-a122-a396a3cdbaab  base\n",
      "caffe_1.0-py3.6                7bb3dbe2-da6e-4145-918d-b6d84aa93b6b  base\n",
      "cuda-py3.6                     82c79ece-4d12-40e6-8787-a7b9e0f62770  base\n",
      "tensorflow_1.15-py3.6-horovod  8964680e-d5e4-5bb8-919b-8342c6c0dfd8  base\n",
      "hybrid_0.1                     8c1a58c6-62b5-4dc4-987a-df751c2756b6  base\n",
      "pytorch-onnx_1.3-py3.7         8d5d8a87-a912-54cf-81ec-3914adaa988d  base\n",
      "caffe-ibm_1.0-py3.6            8d863266-7927-4d1e-97d7-56a7f4c0a19b  base\n",
      "spss-modeler_17.1              902d0051-84bd-4af6-ab6b-8f6aa6fdeabb  base\n",
      "do_12.10                       9100fd72-8159-4eb9-8a0b-a87e12eefa36  base\n",
      "do_py3.7                       9447fa8b-2051-4d24-9eef-5acb0e3c59f8  base\n",
      "spark-mllib_3.0-r_3.6          94bb6052-c837-589d-83f1-f4142f219e32  base\n",
      "cuda-py3.7                     9a44990c-1aa1-4c7d-baf8-c4099011741c  base\n",
      "hybrid_0.2                     9b3f9040-9cee-4ead-8d7a-780600f542f7  base\n",
      "autoai-kb_3.3-py3.7            a545cca3-02df-5c61-9e88-998b09dc79af  base\n",
      "autoai-obm_2.0 with Spark 3.0  af10f35f-69fa-5d66-9bf5-acb58434263a  base\n",
      "tensorflow_2.1-py3.7           c4032338-2a40-500a-beef-b01ab2667e27  base\n",
      "autoai-kb_3.0-py3.6            d139f196-e04b-5d8b-9140-9a10ca1fa91a  base\n",
      "-----------------------------  ------------------------------------  ----\n",
      "Note: Only first 50 records were displayed. To display more use 'limit' parameter.\n"
     ]
    }
   ],
   "source": [
    "client.software_specifications.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e4429883-c883-42b6-87a8-f419d64088cd'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "software_space_uid=client.software_specifications.get_uid_by_name(\"default_py3.7\")\n",
    "software_space_uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Warnings!! :  Model type keras_2.2.4 is deprecated. We recommend you use a supported model type. See Supported Frameworks https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/pm_service_supported_frameworks.html\n"
     ]
    }
   ],
   "source": [
    "model_details = client.repository.store_model(model='image-classification-model_new.tgz',meta_props={\n",
    "    client.repository.ModelMetaNames.NAME:\"AI_based_Natural_disaster_analysis\",\n",
    "    client.repository.ModelMetaNames.TYPE:\"keras_2.2.4\",\n",
    "    client.repository.ModelMetaNames.SOFTWARE_SPEC_UID:software_space_uid}\n",
    "                                            )\n",
    "model_id = client.repository.get_model_uid(model_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5337af53-bbf5-49b8-aaf1-9335df39c5a4'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model content to file: 'model.tar.gz'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/wsuser/work/model.tar.gz'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.repository.download(model_id,'model.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import h5py\n",
    "\n",
    "# Initialization\n",
    "\n",
    "loaded_model = keras.models.load_model('disaster.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your data file was loaded into a botocore.response.StreamingBody object.\n",
    "# Please read the documentation of ibm_boto3 and pandas to learn more about the possibilities to load the data.\n",
    "# ibm_boto3 documentation: https://ibm.github.io/ibm-cos-sdk-python/\n",
    "# pandas documentation: http://pandas.pydata.org/\n",
    "streaming_body_11 = client_7d01c3a53d364926a1681543db957042.get_object(Bucket='imageclassification-donotdelete-pr-r0zoamtbaszqeg', Key='cyclone.jpg')['Body']\n",
    "# add missing __iter__ method so pandas accepts body as file-like object\n",
    "if not hasattr(streaming_body_11, \"__iter__\"): streaming_body_11.__iter__ = types.MethodType( __iter__, streaming_body_11 ) \n",
    "img = image.load_img(streaming_body_11,target_size=(64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x=image.img_to_array(img)\n",
    "x =np.expand_dims(x,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=classifier.predict_classes(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cyclone'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index=['Cyclone','Earthquake','Flood','Wildfire']\n",
    "result=str(index[pred[0]])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
